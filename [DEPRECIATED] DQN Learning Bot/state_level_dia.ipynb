{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f464f03f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mF\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\yousy\\Documents\\Work\\dia\\Wallstreetbets\\.venv\\lib\\site-packages\\torch\\nn\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# mypy: allow-untyped-defs\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparameter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# usort: skip\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     Buffer \u001B[38;5;28;01mas\u001B[39;00m Buffer,\n\u001B[0;32m      4\u001B[0m     Parameter \u001B[38;5;28;01mas\u001B[39;00m Parameter,\n\u001B[0;32m      5\u001B[0m     UninitializedBuffer \u001B[38;5;28;01mas\u001B[39;00m UninitializedBuffer,\n\u001B[0;32m      6\u001B[0m     UninitializedParameter \u001B[38;5;28;01mas\u001B[39;00m UninitializedParameter,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# usort: skip # noqa: F403\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     10\u001B[0m     attention \u001B[38;5;28;01mas\u001B[39;00m attention,\n\u001B[0;32m     11\u001B[0m     functional \u001B[38;5;28;01mas\u001B[39;00m functional,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m     utils \u001B[38;5;28;01mas\u001B[39;00m utils,\n\u001B[0;32m     17\u001B[0m )\n",
      "File \u001B[1;32mc:\\Users\\yousy\\Documents\\Work\\dia\\Wallstreetbets\\.venv\\lib\\site-packages\\torch\\nn\\parameter.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcollections\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OrderedDict\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _disabled_torch_function_impl\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m_ParameterMeta\u001B[39;00m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_TensorMeta):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _C: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define experience tuple structure\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer to store and sample agent experiences\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
    "        experiences = random.sample(self.memory, k=min(self.batch_size, len(self.memory)))\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class QTable:\n",
    "    \"\"\"Tabular Q-learning implementation for real estate decision making\"\"\"\n",
    "\n",
    "    def __init__(self, state_bins, action_size, learning_rate=0.1, discount_factor=0.99):\n",
    "        \"\"\"Initialize Q-table parameters\n",
    "\n",
    "        Args:\n",
    "            state_bins (dict): Dictionary of feature names to number of bins\n",
    "            action_size (int): Number of possible actions\n",
    "            learning_rate (float): Learning rate alpha\n",
    "            discount_factor (float): Discount factor gamma\n",
    "        \"\"\"\n",
    "        self.state_bins = state_bins\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Create Q-table with dimensions based on state bins\n",
    "        dimensions = list(state_bins.values()) + [action_size]\n",
    "        self.q_table = np.zeros(dimensions)\n",
    "\n",
    "        # Track feature names for discretization\n",
    "        self.feature_names = list(state_bins.keys())\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete state for Q-table\"\"\"\n",
    "        if isinstance(state, np.ndarray) and state.dtype == np.dtype('O'):\n",
    "            state = np.vstack(state).astype(np.float32)\n",
    "\n",
    "        # Ensure state is 1-dimensional\n",
    "        if len(state.shape) > 1:\n",
    "            state = state.flatten()\n",
    "\n",
    "        # Use fewer dimensions to make Q-table manageable\n",
    "        # Select most important features (first 5 features)\n",
    "        if len(state) > 5:\n",
    "            state = state[:5]\n",
    "\n",
    "        # Discretize each dimension\n",
    "        discrete_state = []\n",
    "        for i, val in enumerate(state):\n",
    "            # Check for NaN values before conversion\n",
    "            if np.isnan(val):\n",
    "                bin_idx = 0  # Default bin for NaN values\n",
    "            else:\n",
    "                bin_idx = min(max(0, int(val * self.state_bins)), self.state_bins - 1)\n",
    "            discrete_state.append(bin_idx)\n",
    "\n",
    "        return tuple(discrete_state)\n",
    "\n",
    "\n",
    "\n",
    "    def get_value(self, state, action=None):\n",
    "        \"\"\"Get Q-value for state and action, or all action values if action=None\"\"\"\n",
    "        if action is None:\n",
    "            return self.q_table[state]\n",
    "        return self.q_table[state + (action,)]\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-value using Q-learning update rule\"\"\"\n",
    "        current_q = self.q_table[state + (action,)]\n",
    "        max_next_q = np.max(self.q_table[next_state])\n",
    "\n",
    "        # Q-learning update formula: Q(s,a) = Q(s,a) + α[r + γ*max_a'Q(s',a') - Q(s,a)]\n",
    "        new_q = current_q + self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next_q - current_q)\n",
    "\n",
    "        self.q_table[state + (action,)] = new_q\n",
    "\n",
    "class RealEstateAgent:\n",
    "    \"\"\"Agent that interacts with and learns from the real estate environment using Q-Learning\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=42):\n",
    "        \"\"\"Initialize a Q-Learning Agent object\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-table discretization parameters\n",
    "        self.state_bins = 10  # Number of bins per state dimension\n",
    "\n",
    "        # Create Q-table (discretized state space)\n",
    "        # Using a dictionary for sparse representation\n",
    "        self.q_table = {}\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.alpha = 0.1          # Learning rate\n",
    "        self.gamma = 0.99         # Discount factor\n",
    "        self.epsilon = 1.0        # Exploration rate\n",
    "        self.epsilon_decay = 0.995  # Decay rate\n",
    "        self.epsilon_min = 0.01   # Minimum exploration rate\n",
    "\n",
    "        # Portfolio tracking (unchanged)\n",
    "        self.portfolio = {}\n",
    "        self.cash = 1000000\n",
    "        self.net_worth_history = []\n",
    "        self.net_worth_threshold_1 = 2000000\n",
    "        self.net_worth_threshold_2 = 5000000\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete state for Q-table\"\"\"\n",
    "        if isinstance(state, np.ndarray) and state.dtype == np.dtype('O'):\n",
    "            state = np.vstack(state).astype(np.float32)\n",
    "\n",
    "        # Ensure state is 1-dimensional\n",
    "        if len(state.shape) > 1:\n",
    "            state = state.flatten()\n",
    "\n",
    "        # Use fewer dimensions to make Q-table manageable\n",
    "        # Select most important features (first 5 features)\n",
    "        if len(state) > 5:\n",
    "            state = state[:5]\n",
    "\n",
    "        # Discretize each dimension\n",
    "        discrete_state = []\n",
    "        for i, val in enumerate(state):\n",
    "            # Replace NaN with 0 using NumPy's nan_to_num\n",
    "            val = np.nan_to_num(val)\n",
    "            bin_idx = min(max(0, int(val * self.state_bins)), self.state_bins - 1)\n",
    "            discrete_state.append(bin_idx)\n",
    "\n",
    "        return tuple(discrete_state)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-values using Q-Learning update rule\"\"\"\n",
    "        # Discretize states for Q-table lookup\n",
    "        discrete_state = self.discretize_state(state)\n",
    "        discrete_next_state = self.discretize_state(next_state)\n",
    "\n",
    "        # Initialize Q-values if states not seen before\n",
    "        if discrete_state not in self.q_table:\n",
    "            self.q_table[discrete_state] = np.zeros(self.action_size)\n",
    "\n",
    "        if discrete_next_state not in self.q_table:\n",
    "            self.q_table[discrete_next_state] = np.zeros(self.action_size)\n",
    "\n",
    "        # Q-Learning update formula: Q(s,a) ← Q(s,a) + α[r + γ·max(Q(s',a')) - Q(s,a)]\n",
    "        best_next_action = np.argmax(self.q_table[discrete_next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[discrete_next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[discrete_state][action]\n",
    "        self.q_table[discrete_state][action] += self.alpha * td_error\n",
    "\n",
    "        # Decay exploration rate\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def act(self, state, eval_mode=False):\n",
    "        \"\"\"Select action based on current policy (epsilon-greedy)\"\"\"\n",
    "        # Discretize state for Q-table lookup\n",
    "        discrete_state = self.discretize_state(state)\n",
    "\n",
    "        # Set exploration probability\n",
    "        if eval_mode:\n",
    "            epsilon = 0.0  # No exploration during evaluation\n",
    "        else:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            # Exploitation: choose best action from Q-table\n",
    "            if discrete_state in self.q_table:\n",
    "                return np.argmax(self.q_table[discrete_state])\n",
    "            else:\n",
    "                # If state not in Q-table, initialize and choose randomly\n",
    "                self.q_table[discrete_state] = np.zeros(self.action_size)\n",
    "                return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            # Exploration: choose random action\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def update_portfolio(self, prop_id, action, price, year):\n",
    "        \"\"\"\n",
    "        Update the agent's portfolio based on the action taken\n",
    "\n",
    "        Args:\n",
    "            prop_id: The property identifier\n",
    "            action: 'buy' or 'sell'\n",
    "            price: The transaction price\n",
    "            year: The current year in the simulation\n",
    "\n",
    "        Returns:\n",
    "            If selling: returns the profit/loss amount\n",
    "            If buying: returns True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if action == 'buy':\n",
    "            # Check if we have enough cash\n",
    "            if self.cash < price:\n",
    "                return False\n",
    "\n",
    "            # Add property to portfolio with recommended holding period\n",
    "            self.portfolio[prop_id] = {\n",
    "                'purchase_price': price,\n",
    "                'purchase_year': year,\n",
    "                'recommended_holding_period': random.randint(3, 8)  # Random holding period\n",
    "            }\n",
    "\n",
    "            # Deduct price from cash\n",
    "            self.cash -= price\n",
    "            return True\n",
    "\n",
    "        elif action == 'sell':\n",
    "            # Check if we own the property\n",
    "            if prop_id not in self.portfolio:\n",
    "                return 0  # No profit if we don't own the property\n",
    "\n",
    "            # Calculate profit\n",
    "            purchase_price = self.portfolio[prop_id]['purchase_price']\n",
    "            profit = price - purchase_price\n",
    "\n",
    "            # Add sale price to cash\n",
    "            self.cash += price\n",
    "\n",
    "            # Remove property from portfolio\n",
    "            del self.portfolio[prop_id]\n",
    "            return profit\n",
    "\n",
    "        return 0  # Default return for invalid actions\n",
    "\n",
    "\n",
    "    def record_net_worth(self, property_values):\n",
    "        \"\"\"\n",
    "        Calculate and record the agent's net worth based on cash and property values\n",
    "\n",
    "        Args:\n",
    "            property_values (dict): Dictionary mapping property IDs to their current values\n",
    "\n",
    "        Returns:\n",
    "            float: The agent's current net worth\n",
    "        \"\"\"\n",
    "        # Calculate total property value\n",
    "        total_property_value = sum(property_values.values())\n",
    "\n",
    "        # Calculate net worth (cash + property value)\n",
    "        net_worth = self.cash + total_property_value\n",
    "\n",
    "        # Record in history\n",
    "        self.net_worth_history.append(net_worth)\n",
    "\n",
    "        return net_worth\n",
    "\n",
    "    def display_portfolio_details(self, data_processor):\n",
    "        \"\"\"\n",
    "        Display detailed information about the agent's portfolio\n",
    "\n",
    "        Args:\n",
    "            data_processor: The data processor object to get property information\n",
    "        \"\"\"\n",
    "        if not self.portfolio:\n",
    "            print(\"Portfolio is empty.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nPortfolio Details:\")\n",
    "        print(f\"Total Properties: {len(self.portfolio)}\")\n",
    "        print(f\"Cash: ${self.cash:,.2f}\")\n",
    "\n",
    "        total_invested = 0\n",
    "        for prop_id, details in self.portfolio.items():\n",
    "            total_invested += details['purchase_price']\n",
    "\n",
    "        print(f\"Total Invested: ${total_invested:,.2f}\")\n",
    "\n",
    "        if self.net_worth_history:\n",
    "            print(f\"Current Net Worth: ${self.net_worth_history[-1]:,.2f}\")\n",
    "            initial_worth = self.net_worth_history[0] if len(self.net_worth_history) > 1 else 1000000\n",
    "            profit = self.net_worth_history[-1] - initial_worth\n",
    "            print(f\"Profit: ${profit:,.2f} ({profit/initial_worth:.2%})\")\n",
    "\n",
    "        print(\"\\nProperty Details:\")\n",
    "        for prop_id, details in self.portfolio.items():\n",
    "            purchase_price = details['purchase_price']\n",
    "            purchase_year = details['purchase_year']\n",
    "            holding_period = details.get('recommended_holding_period', 5)\n",
    "            print(f\"Property {prop_id}: Purchased for ${purchase_price:,.2f} in year {purchase_year}, Recommended holding: {holding_period} years\")\n",
    "\n",
    "\n",
    "class RealEstateDataProcessor:\n",
    "    \"\"\"Class to handle real estate data loading, processing and feature engineering\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir='./'):\n",
    "        \"\"\"Initialize the data processor\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory containing the dataset files\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.scaler = None\n",
    "        self.datasets = {}\n",
    "        self.property_sets = {}  # For train/val/test/prod splits\n",
    "        self.last_updated_years = {}  # Track when each property was last updated\n",
    "        self.property_values_by_year = {}  # Track property values by year\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load all dataset files\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "\n",
    "        try:\n",
    "            self.datasets = {}\n",
    "\n",
    "            self.datasets['mortgage_info'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_mortgage_info.csv')))\n",
    "            self.datasets['nearby_homes'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_nearby_homes.csv')))\n",
    "            self.datasets['price_history'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_price_history.csv')))\n",
    "            self.datasets['schools_info'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_schools_info.csv')))\n",
    "            self.datasets['subtype'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_subtype.csv')))\n",
    "            self.datasets['tax_info'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_tax_info.csv')))\n",
    "            self.datasets['property_listings'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'property_listings.csv')))\n",
    "\n",
    "            print(\"All datasets loaded successfully!\")\n",
    "            print(f\"Main property dataset shape: {self.datasets['property_listings'].shape}\")\n",
    "\n",
    "            # Extract historical years from price_history\n",
    "            self.datasets['price_history']['dateOfEvent'] = pd.to_datetime(self.datasets['price_history']['dateOfEvent'])\n",
    "            price_history_years = self.datasets['price_history'][\n",
    "                (self.datasets['price_history']['dateOfEvent'].dt.year >= 2010) &\n",
    "                (self.datasets['price_history']['dateOfEvent'].dt.year <= 2024)\n",
    "            ]\n",
    "            \n",
    "            # Group by property and count unique years\n",
    "            prop_year_counts = price_history_years.groupby('zpid')['dateOfEvent'] \\\n",
    "                .apply(lambda x: x.dt.year.nunique())\n",
    "            \n",
    "            min_required_years = 7\n",
    "            \n",
    "            # Filter properties with at least min_required_years years of data \n",
    "            valid_properties = prop_year_counts[prop_year_counts >= min_required_years].index.tolist()\n",
    "\n",
    "            # If still no valid properties, use any with at least 1 year\n",
    "            if len(valid_properties) == 0:\n",
    "                valid_properties = price_history_years['zpid'].unique()\n",
    "                print(f\"Using {len(valid_properties)} properties with any historical data\")\n",
    "            \n",
    "            # Then proceed with intersection check\n",
    "            valid_properties = np.intersect1d(\n",
    "                valid_properties,\n",
    "                self.datasets['property_listings']['zpid'].unique()\n",
    "            )\n",
    "            \n",
    "            # Additional fallback if still no properties\n",
    "            if valid_properties.size == 0:\n",
    "                print(\"No properties with any price history found - using all properties\")\n",
    "                valid_properties = self.datasets['property_listings']['zpid'].unique()\n",
    "\n",
    "            \n",
    "            # Intersection with property listings\n",
    "            valid_properties = np.intersect1d(\n",
    "                valid_properties,\n",
    "                self.datasets['property_listings']['zpid'].unique()\n",
    "            )\n",
    "            \n",
    "            valid_properties = np.array(valid_properties)\n",
    "            \n",
    "            if valid_properties.size == 0:\n",
    "                # Fallback: Use properties with ANY 2010-2024 history if none have full 15 years\n",
    "                valid_properties = price_history_years['zpid'].unique()\n",
    "                print(f\"Warning: No properties with full 2010-2024 history. Using {len(valid_properties)} with partial data\")\n",
    "\n",
    "            # Then filter all datasets to only include these properties\n",
    "            for dataset_name in self.datasets:\n",
    "                if isinstance(self.datasets[dataset_name], pd.DataFrame) and 'zpid' in self.datasets[dataset_name].columns:\n",
    "                    self.datasets[dataset_name] = self.datasets[dataset_name][\n",
    "                        self.datasets[dataset_name]['zpid'].isin(valid_properties)]\n",
    "\n",
    "\n",
    "            print(f\"Valid Properties: {len(valid_properties)}\")\n",
    "            # Filter for years 2000 and later\n",
    "            valid_years = sorted(price_history_years['zpid'].unique())\n",
    "            valid_years = [year for year in valid_years if year >= 2010 and year <= 2024]\n",
    "            print(f\"Filtered to {len(valid_years)} properties with price history in 2010\")\n",
    "\n",
    "            # Extract current property years from lastUpdated\n",
    "            self.datasets['property_listings']['year'] = pd.to_datetime(\n",
    "                self.datasets['property_listings']['lastUpdated']).dt.year\n",
    "\n",
    "            # Create a mapping of zpid to all its available years from both datasets\n",
    "            property_years = {}\n",
    "\n",
    "            # First add years from price_history (historical data)\n",
    "            for zpid, year in zip(self.datasets['price_history']['zpid'],\n",
    "                                  self.datasets['price_history']['dateOfEvent'].dt.year):\n",
    "                if year >= 2000:  # Only include years from 2000 onwards\n",
    "                    if zpid not in property_years:\n",
    "                        property_years[zpid] = set()\n",
    "                    property_years[zpid].add(year)\n",
    "\n",
    "            # Then add current years from property_listings\n",
    "            for zpid, year in zip(self.datasets['property_listings']['zpid'],\n",
    "                                  self.datasets['property_listings']['year']):\n",
    "                if zpid in property_years:  # Only add if property exists in historical data\n",
    "                    property_years[zpid].add(year)\n",
    "\n",
    "            # Use all years from 2000 to present\n",
    "            self.years = sorted([year for year in valid_years if year >= 2010])\n",
    "\n",
    "            if self.years:\n",
    "                print(f\"Dataset spans years: {min(self.years)} to {max(self.years)}\")\n",
    "                print(f\"Total years available: {len(self.years)}\")\n",
    "            else:\n",
    "                print(\"No valid years found in the dataset.\")\n",
    "\n",
    "\n",
    "            return True\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading datasets: {e}\")\n",
    "            print(\"Please ensure all required CSV files are in the specified directory.\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def split_data_by_properties(self):\n",
    "        \"\"\"Split data by properties while maintaining the full timeline\n",
    "        Uses property-based split instead of time-based split.\n",
    "        Only considers 30% of properties for all processing.\n",
    "        Training set only includes properties from 5 randomly chosen states.\n",
    "        \"\"\"\n",
    "        if not self.datasets:\n",
    "            print(\"Datasets not loaded. Call load_datasets() first.\")\n",
    "            return\n",
    "\n",
    "        # Check if property_listings exists\n",
    "        if 'property_listings' not in self.datasets:\n",
    "            print(\"Error: 'property_listings' dataset not found. Make sure the CSV file exists and was loaded correctly.\")\n",
    "            return\n",
    "\n",
    "        # Check if 'zpid' column exists in property_listings\n",
    "        if 'zpid' not in self.datasets['property_listings'].columns:\n",
    "            print(\"Error: 'zpid' column not found in 'property_listings'. Check the column names in your CSV file.\")\n",
    "            print(f\"Available columns: {self.datasets['property_listings'].columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        # Filter price history to Only 2010 entries\n",
    "        valid_price_history = self.datasets['price_history'][\n",
    "            self.datasets['price_history']['dateOfEvent'].dt.year == 2010\n",
    "        ]\n",
    "        \n",
    "        # Get properties with both 2010 price history AND current listings\n",
    "        valid_properties = np.intersect1d(\n",
    "            valid_price_history['zpid'].unique(),\n",
    "            self.datasets['property_listings']['zpid'].unique()\n",
    "        )\n",
    "        \n",
    "        if not valid_properties.size:\n",
    "            raise ValueError(\"No properties with 2010 price history found\")\n",
    "        \n",
    "        # Get properties with price history 2010-2024 (renamed variable for clarity)\n",
    "        # price_history = self.datasets['price_history'].copy()\n",
    "        # price_history['dateOfEvent'] = pd.to_datetime(price_history['dateOfEvent'])\n",
    "        \n",
    "        # valid_price_history = price_history[\n",
    "        #     (price_history['dateOfEvent'].dt.year >= 2010) &\n",
    "        #     (price_history['dateOfEvent'].dt.year <= 2024)\n",
    "        # ]\n",
    "        # \n",
    "        # # Get intersection of properties with both listing info and price history\n",
    "        # valid_properties = np.intersect1d(\n",
    "        #     valid_price_history['zpid'].unique(),\n",
    "        #     self.datasets['property_listings']['zpid'].unique()\n",
    "        # )\n",
    "        # \n",
    "        # # After getting valid_properties from np.intersect1d()\n",
    "        # if not len(valid_properties):\n",
    "        #     raise ValueError(\"No valid properties with price history 2010-2024 found\")\n",
    "        \n",
    "        print(f\"Valid properties: {len(valid_properties)}\")\n",
    "        # Rename variable for clarity\n",
    "        all_properties = valid_properties  # Only after validation\n",
    "\n",
    "        # # Check if 'yearBuilt' column exists\n",
    "        # is_2010 = self.datasets['property_listings']['yearBuilt'] == 2010\n",
    "        # if 'yearBuilt' not in self.datasets['property_listings'].columns:\n",
    "        #     print(\"Warning: 'yearBuilt' column not found in 'property_listings'. Skipping year-based filtering.\")\n",
    "        #     all_properties = self.datasets['property_listings']['zpid'].unique()\n",
    "        # else:\n",
    "        #     # Get all unique property IDs\n",
    "        #     all_properties = self.datasets['property_listings']['zpid'].unique()\n",
    "        # \n",
    "        #     # In split_data_by_properties():\n",
    "        #     if 'yearBuilt' in self.datasets['property_listings'].columns:\n",
    "        #         valid_properties = self.datasets['property_listings'].loc[\n",
    "        #             self.datasets['property_listings']['yearBuilt'] == 2010, 'zpid'\n",
    "        #         ].unique()\n",
    "        # \n",
    "        #         if len(valid_properties) == 0:\n",
    "        #             print(\"No 2010 properties found, using all properties\")\n",
    "        #             valid_properties = self.datasets['property_listings']['zpid'].unique()\n",
    "        #     else:\n",
    "        #         valid_properties = self.datasets['property_listings']['zpid'].unique()\n",
    "\n",
    "        # Shuffle the properties for random assignment\n",
    "        np.random.shuffle(all_properties)\n",
    "\n",
    "        # Select only 30% of properties for consideration\n",
    "        considered_count = int(len(all_properties) * 0.3)\n",
    "        considered_properties = all_properties[:considered_count]\n",
    "        print(f\"Considering only {considered_count} properties ({considered_count/len(all_properties):.1%} of total)\")\n",
    "\n",
    "        # Check if 'state' column exists\n",
    "        if 'state' not in self.datasets['property_listings'].columns:\n",
    "            print(\"Warning: 'state' column not found in 'property_listings'. Using random assignment instead.\")\n",
    "            # Assign random \"states\" for illustration\n",
    "            property_to_state = {prop_id: f\"State_{i%10}\" for i, prop_id in enumerate(considered_properties)}\n",
    "        else:\n",
    "            # Create a mapping of property IDs to their states\n",
    "            property_to_state = {}\n",
    "            for prop_id, state in zip(self.datasets['property_listings']['zpid'],\n",
    "                                    self.datasets['property_listings']['state']):\n",
    "                if prop_id in considered_properties:\n",
    "                    property_to_state[prop_id] = state\n",
    "\n",
    "        # Get unique states from considered properties\n",
    "        available_states = list(set(property_to_state.values()))\n",
    "        print(f\"Available states in considered properties: {len(available_states)}\")\n",
    "\n",
    "        # Randomly select 5 states for training (or fewer if there are less than 5 states)\n",
    "        training_states = random.sample(available_states, min(5, len(available_states)))\n",
    "        print(f\"Selected states for training: {training_states}\")\n",
    "\n",
    "        # Calculate sizes for splits\n",
    "        train_size = int(len(considered_properties) * 0.6)\n",
    "        val_size = int(len(considered_properties) * 0.15)\n",
    "        test_size = int(len(considered_properties) * 0.15)\n",
    "\n",
    "        # Filter considered properties by state for training\n",
    "        train_state_properties = [p for p in considered_properties if property_to_state.get(p) in training_states]\n",
    "\n",
    "        # Shuffle the filtered properties\n",
    "        np.random.shuffle(train_state_properties)\n",
    "\n",
    "        # Select properties for training (up to the original train size)\n",
    "        train_props = train_state_properties[:min(train_size, len(train_state_properties))]\n",
    "\n",
    "        # Select remaining properties for val, test, prod from all considered properties\n",
    "        remaining_props = [p for p in considered_properties if p not in train_props]\n",
    "        np.random.shuffle(remaining_props)\n",
    "\n",
    "        self.property_sets = {\n",
    "            'train': np.array(train_props),\n",
    "            'val': np.array(remaining_props[:min(val_size, len(remaining_props))]),\n",
    "            'test': np.array(remaining_props[val_size:min(val_size + test_size, len(remaining_props))]),\n",
    "            'prod': np.array(remaining_props[min(val_size + test_size, len(remaining_props)):])\n",
    "        }\n",
    "\n",
    "        # Print statistics\n",
    "        for split, props in self.property_sets.items():\n",
    "            print(f\"{split} set: {len(props)} properties ({len(props)/len(all_properties) if len(all_properties) > 0 else 0:.1%} of total)\")\n",
    "\n",
    "        # Print states distribution for training\n",
    "        train_states_used = [property_to_state.get(prop_id) for prop_id in self.property_sets['train']]\n",
    "        train_states_count = {state: train_states_used.count(state) for state in set(train_states_used)}\n",
    "        print(f\"Training set states distribution: {train_states_count}\")\n",
    "\n",
    "        return self.property_sets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def engineer_features(self):\n",
    "        \"\"\"Engineer features with focus on historical price data integration\"\"\"\n",
    "        if not self.datasets or not self.property_sets:\n",
    "            print(\"Datasets not loaded or not split. Call load_datasets() and split_data_by_properties() first.\")\n",
    "            return\n",
    "\n",
    "        print(\"Engineering features with historical data integration...\")\n",
    "\n",
    "        # Get all properties from our splits (the 30% considered)\n",
    "        considered_properties = np.concatenate(list(self.property_sets.values()))\n",
    "\n",
    "        # 1. Start with price history to create a base DataFrame with historical data\n",
    "        price_history = self.datasets['price_history'].copy()\n",
    "        # Filter to only include our considered properties\n",
    "        price_history = price_history[price_history['zpid'].isin(considered_properties)]\n",
    "\n",
    "        price_history['dateOfEvent'] = pd.to_datetime(price_history['dateOfEvent'])\n",
    "        price_history['year'] = price_history['dateOfEvent'].dt.year\n",
    "\n",
    "        # Filter for years 2000 and later\n",
    "        price_history = price_history[price_history['year'] >= 2000]\n",
    "\n",
    "        # Group by zpid and year to get price per year (if multiple events per year)\n",
    "        yearly_prices = price_history.groupby(['zpid', 'year'])['price'].mean().reset_index()\n",
    "\n",
    "        # Calculate year-over-year price changes\n",
    "        yearly_prices = yearly_prices.sort_values(['zpid', 'year'])\n",
    "        yearly_prices['prev_price'] = yearly_prices.groupby('zpid')['price'].shift(1)\n",
    "        yearly_prices['price_yoy_change'] = (yearly_prices['price'] - yearly_prices['prev_price']) / yearly_prices['prev_price']\n",
    "\n",
    "        # 2. Get property information from property_listings\n",
    "        properties = self.datasets['property_listings'].copy()\n",
    "        # Filter to only include considered properties\n",
    "        properties = properties[properties['zpid'].isin(considered_properties)]\n",
    "\n",
    "        properties['lastUpdated'] = pd.to_datetime(properties['lastUpdated'])\n",
    "        properties['year_updated'] = properties['lastUpdated'].dt.year\n",
    "\n",
    "        # 3. Create a base DataFrame with all property-year combinations\n",
    "        # Use only our considered properties\n",
    "        all_properties = considered_properties\n",
    "\n",
    "        # Get all years from 2000 to present\n",
    "        year_updated_max = properties['year_updated'].max()\n",
    "        price_history_max = price_history['year'].max()\n",
    "\n",
    "        # Handle potential NaN values\n",
    "        if pd.isna(year_updated_max):\n",
    "            year_updated_max = 2000  # Default fallback value\n",
    "        if pd.isna(price_history_max):\n",
    "            price_history_max = 2000  # Default fallback value\n",
    "\n",
    "        all_years = list(range(2000, int(max(year_updated_max, price_history_max)) + 1))\n",
    "\n",
    "        # Create a cross-product of properties and years\n",
    "        property_years = []\n",
    "        for prop in all_properties:\n",
    "            for year in all_years:\n",
    "                property_years.append({'zpid': prop, 'year': year})\n",
    "\n",
    "        # Create base DataFrame\n",
    "        df = pd.DataFrame(property_years)\n",
    "\n",
    "        # 4. Merge property information\n",
    "        # First get the latest property info\n",
    "        latest_properties = properties.sort_values('lastUpdated').groupby('zpid').last().reset_index()\n",
    "\n",
    "        # Merge with base DataFrame\n",
    "        print(df.columns)\n",
    "        df = pd.merge(df, latest_properties[['zpid', 'price', 'livingArea', 'bedrooms', 'bathrooms',\n",
    "                                          'yearBuilt', 'homeType', 'homeStatus', 'state',\n",
    "                                          'pageViewCount', 'favoriteCount']],\n",
    "                    on='zpid', how='left')\n",
    "\n",
    "        # 5. Merge historical prices\n",
    "        df = pd.merge(df, yearly_prices[['zpid', 'year', 'price_yoy_change']],\n",
    "                    on=['zpid', 'year'], how='left')\n",
    "\n",
    "        # For historical years, update property price using the price from price_history\n",
    "        historical_prices = pd.merge(df[['zpid', 'year']],\n",
    "                                    yearly_prices[['zpid', 'year', 'price']],\n",
    "                                    on=['zpid', 'year'], how='left')\n",
    "\n",
    "        # Update prices where historical data exists\n",
    "        mask = historical_prices['price'].notna()\n",
    "        if mask.any():\n",
    "            df.loc[mask, 'price'] = historical_prices.loc[mask, 'price'].values\n",
    "\n",
    "        # 6. Add basic features\n",
    "        df.loc[:, 'price_per_sqft'] = df['price'] / df['livingArea']\n",
    "        df.loc[:, 'age'] = df['year'] - df['yearBuilt']\n",
    "        df.loc[:, 'bed_bath_ratio'] = df['bedrooms'] / df['bathrooms']\n",
    "\n",
    "        # 7. Add tax and mortgage features\n",
    "        tax_info = self.datasets['tax_info'][['zpid', 'taxPaid', 'valueIncreaseRate']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        tax_info = tax_info[tax_info['zpid'].isin(considered_properties)]\n",
    "        df = pd.merge(df, tax_info, on='zpid', how='left')\n",
    "\n",
    "        mortgage_info = self.datasets['mortgage_info'][['zpid', 'rate']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        mortgage_info = mortgage_info[mortgage_info['zpid'].isin(considered_properties)]\n",
    "        df = pd.merge(df, mortgage_info, on='zpid', how='left')\n",
    "\n",
    "        # 8. Add school ratings\n",
    "        schools = self.datasets['schools_info'][['zpid', 'schoolRating']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        schools = schools[schools['zpid'].isin(considered_properties)]\n",
    "        school_ratings = schools.groupby('zpid')['schoolRating'].mean().reset_index()\n",
    "        school_ratings.rename(columns={'schoolRating': 'avg_school_rating'}, inplace=True)\n",
    "        df = pd.merge(df, school_ratings, on='zpid', how='left')\n",
    "\n",
    "        # 9. Add nearby home comparison\n",
    "        nearby = self.datasets['nearby_homes'][['zpid', 'priceComp']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        nearby = nearby[nearby['zpid'].isin(considered_properties)]\n",
    "        nearby_avg_price = nearby.groupby('zpid')['priceComp'].mean().reset_index()\n",
    "        nearby_avg_price.rename(columns={'priceComp': 'nearby_avg_price'}, inplace=True)\n",
    "        df = pd.merge(df, nearby_avg_price, on='zpid', how='left')\n",
    "        df.loc[:, 'price_vs_nearby'] = df['price'] / df['nearby_avg_price']\n",
    "\n",
    "        # 10. One-hot encoding\n",
    "        for col in ['homeType', 'homeStatus', 'state']:\n",
    "            if col in df.columns:\n",
    "                top_values = df[col].value_counts().nlargest(5).index\n",
    "                for val in top_values:\n",
    "                    df.loc[:, f'{col}_{val}'] = (df[col] == val).astype('int8')\n",
    "\n",
    "        # Drop original categorical columns to save memory\n",
    "        df = df.drop(['homeType', 'homeStatus', 'state'], axis=1, errors='ignore')\n",
    "\n",
    "        # 11. Handle missing values\n",
    "        df.fillna({\n",
    "            'price_yoy_change': 0,\n",
    "            'valueIncreaseRate': df['valueIncreaseRate'].median(),\n",
    "            'taxPaid': df['taxPaid'].median(),\n",
    "            'rate': 0.05,  # Default mortgage rate\n",
    "            'avg_school_rating': df['avg_school_rating'].median(),\n",
    "            'nearby_avg_price': df['nearby_avg_price'].median(),\n",
    "            'price_vs_nearby': 1,\n",
    "            'pageViewCount': 0,\n",
    "            'favoriteCount': 0\n",
    "        }, inplace=True)\n",
    "\n",
    "        # 12. Store property values by year for reward calculation\n",
    "        for year in df['year'].unique():\n",
    "            year_data = df[df['year'] == year][['zpid', 'price']]\n",
    "            self.property_values_by_year[year] = dict(zip(year_data['zpid'].values, year_data['price'].values))\n",
    "\n",
    "        # 13. Final cleanup\n",
    "        df.replace([np.inf, -np.inf], np.finfo('float32').max, inplace=True)\n",
    "\n",
    "        # Update main dataframe\n",
    "        self.datasets['processed_data'] = df\n",
    "\n",
    "        print(f\"Feature engineering complete. Final dataframe shape: {df.shape}\")\n",
    "        print(f\"Years available in processed data: {sorted(df['year'].unique())}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def prepare_state_features(self):\n",
    "        \"\"\"Prepare the state features for the DQN model\"\"\"\n",
    "        if 'processed_data' not in self.datasets:\n",
    "            print(\"Features not engineered. Call engineer_features() first.\")\n",
    "            return\n",
    "\n",
    "        df = self.datasets['processed_data']\n",
    "\n",
    "        # Check for and add default values for missing columns\n",
    "        missing_columns = {\n",
    "            'rate': 0.05,  # Default mortgage rate\n",
    "            'pageViewCount': 0,  # Default page views\n",
    "            'favoriteCount': 0   # Default favorites\n",
    "        }\n",
    "\n",
    "        for col, default_val in missing_columns.items():\n",
    "            if col not in df.columns:\n",
    "                print(f\"Warning: Column '{col}' not found, adding with default value {default_val}\")\n",
    "                df[col] = default_val\n",
    "\n",
    "        # Now proceed with feature selection\n",
    "        feature_columns = [\n",
    "            'price', 'livingArea', 'bedrooms', 'bathrooms', 'yearBuilt', 'age',\n",
    "            'price_per_sqft', 'bed_bath_ratio', 'price_yoy_change', 'taxPaid',\n",
    "            'valueIncreaseRate', 'rate', 'avg_school_rating', 'nearby_avg_price',\n",
    "            'price_vs_nearby', 'pageViewCount', 'favoriteCount'\n",
    "        ]\n",
    "        # Add the one-hot encoded columns for homeType, homeStatus, and state\n",
    "        one_hot_columns = [col for col in df.columns if col.startswith(('homeType_', 'homeStatus_', 'state_'))]\n",
    "        feature_columns.extend(one_hot_columns)\n",
    "\n",
    "        # Extract just the feature columns\n",
    "        features_df = df[feature_columns + ['zpid', 'year']]\n",
    "\n",
    "        # Scale the numerical features\n",
    "        numerical_features = [\n",
    "            'price', 'livingArea', 'bedrooms', 'bathrooms', 'age',\n",
    "            'price_per_sqft', 'bed_bath_ratio', 'price_yoy_change', 'taxPaid',\n",
    "            'valueIncreaseRate', 'rate', 'avg_school_rating', 'nearby_avg_price',\n",
    "            'price_vs_nearby', 'pageViewCount', 'favoriteCount'\n",
    "        ]\n",
    "\n",
    "        # Fit scaler on training data only to prevent data leakage\n",
    "        train_features = features_df[features_df['zpid'].isin(self.property_sets['train'])]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train_features[numerical_features])\n",
    "\n",
    "        # Transform all features\n",
    "        features_df[numerical_features] = self.scaler.transform(features_df[numerical_features])\n",
    "\n",
    "        # Store the prepared features\n",
    "        self.datasets['state_features'] = features_df\n",
    "\n",
    "        # Return the feature dimension for the DQN model\n",
    "        self.state_size = len(feature_columns)\n",
    "        print(f\"State features prepared. State dimension: {self.state_size}\")\n",
    "\n",
    "        return features_df, self.state_size\n",
    "\n",
    "    def get_properties_for_year(self, year, split):\n",
    "        \"\"\"Get properties available in a specific year for a specific data split\"\"\"\n",
    "        if 'state_features' not in self.datasets:\n",
    "            print(\"State features not prepared. Call prepare_state_features() first.\")\n",
    "            return None\n",
    "\n",
    "        # Get properties for the given split\n",
    "        split_properties = self.property_sets[split]\n",
    "\n",
    "        # Get data for the given year filtered by split properties\n",
    "        year_data = self.datasets['state_features'][\n",
    "            (self.datasets['state_features']['year'] == year) &\n",
    "            (self.datasets['state_features']['zpid'].isin(split_properties))\n",
    "        ]\n",
    "\n",
    "        return year_data\n",
    "\n",
    "    def get_property_state(self, property_id, year):\n",
    "        \"\"\"Get the state representation for a specific property and year\"\"\"\n",
    "        if 'state_features' not in self.datasets:\n",
    "            print(\"State features not prepared. Call prepare_state_features() first.\")\n",
    "            return None\n",
    "\n",
    "        # Get data for the specific property and year\n",
    "        property_data = self.datasets['state_features'][\n",
    "            (self.datasets['state_features']['zpid'] == property_id) &\n",
    "            (self.datasets['state_features']['year'] == year)\n",
    "        ]\n",
    "\n",
    "        if property_data.empty:\n",
    "            return None\n",
    "\n",
    "        # Return state features (excluding zpid and year)\n",
    "        state_features = property_data.drop(['zpid', 'year'], axis=1).values[0]\n",
    "\n",
    "        return state_features\n",
    "\n",
    "    def get_property_value(self, property_id, year):\n",
    "        \"\"\"Get the value of a property in a specific year\"\"\"\n",
    "        return self.property_values_by_year.get(year, {}).get(property_id, None)\n",
    "\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\"Reduces memory usage of a dataframe by downcasting numeric types.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage before optimization: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Skip categorical columns or convert them to ordered\n",
    "        if isinstance(col_type, CategoricalDtype):\n",
    "            # Option 1: Skip categorical columns entirely\n",
    "            continue\n",
    "\n",
    "            # Option 2: Make categorical columns ordered before operations\n",
    "            # df[col] = df[col].cat.as_ordered()\n",
    "\n",
    "        elif col_type != object:\n",
    "            # Use safe min/max operations with explicit null handling\n",
    "            if df[col].isna().any():  # Check for nulls first\n",
    "                non_null_values = df[col].dropna()\n",
    "                if len(non_null_values) > 0:  # Make sure we have values after dropping nulls\n",
    "                    c_min = non_null_values.min()\n",
    "                    c_max = non_null_values.max()\n",
    "                else:\n",
    "                    # If all values are null, use safe defaults\n",
    "                    c_min = 0\n",
    "                    c_max = 0\n",
    "            else:\n",
    "                # No nulls, proceed normally\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "            # Integer downcasting\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "\n",
    "            # Float downcasting\n",
    "            elif str(col_type)[:5] == 'float':\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        # For object columns that we want to convert to category\n",
    "        elif df[col].nunique() < df.shape[0] * 0.5:\n",
    "            # Handle null values in categorical data\n",
    "            if df[col].isna().any():\n",
    "                # Convert to object type first, fill nulls with placeholder, then convert to category\n",
    "                df[col] = df[col].astype(object).fillna('Unknown').astype('category')\n",
    "            else:\n",
    "                # No nulls, safe to convert directly\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "    print(f'Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_agent(agent, data_processor, split='train', num_epochs=5):\n",
    "    \"\"\"Train the agent using the specified data split\"\"\"\n",
    "    print(f\"Training agent on {split} split for {num_epochs} epochs...\")\n",
    "    years = sorted(data_processor.datasets['price_history']['dateOfEvent'].dt.year.unique())\n",
    "    \n",
    "    # If no historical years, use current year from listings\n",
    "    if len(years) == 0:\n",
    "        years = [data_processor.datasets['property_listings']['year'].max()]\n",
    "    \n",
    "    # Force at least one training year\n",
    "    years = years or [2024]  # Default to 2024 if no years found\n",
    "    \n",
    "    print(f\"Training years: {years}\")\n",
    "    returns_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Reset agent's portfolio at the beginning of each epoch\n",
    "        agent.portfolio = {}\n",
    "        agent.cash = 1000000  # Reset starting cash\n",
    "        agent.net_worth_history = []\n",
    "\n",
    "        total_profit = 0\n",
    "\n",
    "        # For each year in chronological order\n",
    "        for i, year in enumerate(tqdm(years, desc=f\"Processing years\")):\n",
    "            # Get available properties for this year and split\n",
    "            year_data = data_processor.get_properties_for_year(year, split)\n",
    "\n",
    "            if year_data is None or year_data.empty:\n",
    "                print(f\"No data available for year {year}, split {split}\")\n",
    "                continue\n",
    "\n",
    "            # Track properties acted on this year to avoid duplicates\n",
    "            acted_properties = set()\n",
    "\n",
    "            # First, check if any properties in the portfolio should be sold\n",
    "            properties_to_sell = []\n",
    "            for prop_id, details in agent.portfolio.items():\n",
    "                holding_years = year - details['purchase_year']\n",
    "\n",
    "                # Check if we've held the property for the recommended period\n",
    "                if holding_years >= details['recommended_holding_period']:\n",
    "                    properties_to_sell.append(prop_id)\n",
    "\n",
    "            # Sell properties that have reached their holding period\n",
    "            for prop_id in properties_to_sell:\n",
    "                current_value = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "                if current_value is not None:\n",
    "                    # Get property state for the selling decision\n",
    "                    prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "                    if prop_state is not None:\n",
    "                        # Get agent's action (0: hold, 1: sell)\n",
    "                        action = agent.act(prop_state)\n",
    "\n",
    "                        if action == 1:  # Sell\n",
    "                            profit = agent.update_portfolio(prop_id, 'sell', current_value, year)\n",
    "                            total_profit += profit\n",
    "\n",
    "                            # Calculate reward (profit as percentage of purchase price)\n",
    "                            purchase_price = agent.portfolio.get(prop_id, {}).get('purchase_price', current_value)\n",
    "                            reward = profit / purchase_price if purchase_price > 0 else 0\n",
    "\n",
    "                            # Get the next state (could be None at the end of data)\n",
    "                            next_state = prop_state  # Simplified; in real scenario would be updated state\n",
    "\n",
    "                            # Determine if this is the end of the episode\n",
    "                            done = (i == len(years) - 1)\n",
    "\n",
    "                            # Agent learning step\n",
    "                            agent.step(prop_state, np.array([action]), reward, next_state, done)\n",
    "\n",
    "                            acted_properties.add(prop_id)\n",
    "\n",
    "            # Then, consider buying new properties\n",
    "            for _, row in year_data.iterrows():\n",
    "                prop_id = row['zpid']\n",
    "\n",
    "                # Skip if we already acted on this property this year\n",
    "                if prop_id in acted_properties:\n",
    "                    continue\n",
    "\n",
    "                # Get property state\n",
    "                prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "                if prop_state is not None:\n",
    "                    # Get agent's action (0: hold/don't buy, 1: buy)\n",
    "                    action = agent.act(prop_state)\n",
    "\n",
    "                    if action == 1:  # Buy\n",
    "                        property_price = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "                        if property_price is not None and agent.cash >= property_price:\n",
    "                            success = agent.update_portfolio(prop_id, 'buy', property_price, year)\n",
    "\n",
    "                            # Calculate immediate reward (small negative for buying to account for transaction costs)\n",
    "                            reward = -0.01  # Small negative reward for buying\n",
    "\n",
    "                            # Get the next state (could be None at the end of data)\n",
    "                            next_state = prop_state  # Simplified\n",
    "\n",
    "                            # Determine if this is the end of the episode\n",
    "                            done = (i == len(years) - 1)\n",
    "\n",
    "                            # Agent learning step\n",
    "                            agent.step(prop_state, np.array([[action]]), reward, next_state, done)\n",
    "\n",
    "                            acted_properties.add(prop_id)\n",
    "\n",
    "            # Update agent's net worth at the end of each year\n",
    "            current_property_values = {prop_id: data_processor.get_property_value(prop_id, year)\n",
    "                                      for prop_id in agent.portfolio.keys()}\n",
    "            current_property_values = {k: v for k, v in current_property_values.items() if v is not None}\n",
    "\n",
    "            net_worth = agent.record_net_worth(current_property_values)\n",
    "            print(f\"Year {year} - Net Worth: ${net_worth:,.2f}, Cash: ${agent.cash:,.2f}, Properties: {len(agent.portfolio)}\")\n",
    "\n",
    "        if agent.net_worth_history:\n",
    "            epoch_return = agent.net_worth_history[-1] - 1000000\n",
    "        else:\n",
    "            epoch_return = 0\n",
    "            print(\"Warning: No net worth recorded this epoch\")  # Final net worth minus initial cash\n",
    "        returns_history.append(epoch_return)\n",
    "        print(f\"Epoch {epoch+1} Return: ${epoch_return:,.2f}\")\n",
    "\n",
    "        # Display detailed portfolio information after each epoch\n",
    "        agent.display_portfolio_details(data_processor)\n",
    "\n",
    "    return returns_history\n",
    "\n",
    "def evaluate_agent(agent, data_processor, split='test'):\n",
    "    \"\"\"Evaluate the agent on the specified data split\n",
    "\n",
    "    Args:\n",
    "        agent (RealEstateAgent): The trained agent\n",
    "        data_processor (RealEstateDataProcessor): The data processor\n",
    "        split (str): The data split to use ('val', 'test', 'prod')\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating agent on {split} split...\")\n",
    "\n",
    "    years = data_processor.years\n",
    "\n",
    "    # Reset agent's portfolio for evaluation\n",
    "    agent.portfolio = {}\n",
    "    agent.cash = 1000000  # Reset starting cash\n",
    "    agent.net_worth_history = []\n",
    "\n",
    "    total_profit = 0\n",
    "    transactions = []\n",
    "\n",
    "    # For each year in chronological order\n",
    "    for i, year in enumerate(tqdm(years, desc=f\"Evaluating years\")):\n",
    "        # Get available properties for this year and split\n",
    "        year_data = data_processor.get_properties_for_year(year, split)\n",
    "\n",
    "        if year_data is None or year_data.empty:\n",
    "            print(f\"No data available for year {year}, split {split}\")\n",
    "            continue\n",
    "\n",
    "        # Track properties acted on this year to avoid duplicates\n",
    "        acted_properties = set()\n",
    "\n",
    "        # First, check if any properties in the portfolio should be sold\n",
    "        properties_to_sell = []\n",
    "        for prop_id, details in agent.portfolio.items():\n",
    "            holding_years = year - details['purchase_year']\n",
    "\n",
    "            # Check if we've held the property for the recommended period\n",
    "            if holding_years >= details['recommended_holding_period']:\n",
    "                properties_to_sell.append(prop_id)\n",
    "\n",
    "        # Sell properties that have reached their holding period\n",
    "        for prop_id in properties_to_sell:\n",
    "            current_value = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "            if current_value is not None:\n",
    "                # Get property state for the selling decision\n",
    "                prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "                if prop_state is not None:\n",
    "                    # Get agent's action (0: hold, 1: sell) - in eval mode\n",
    "                    action = agent.act(prop_state, eval_mode=True)\n",
    "\n",
    "                    if action == 1:  # Sell\n",
    "                        purchase_price = agent.portfolio[prop_id]['purchase_price']\n",
    "                        purchase_year = agent.portfolio[prop_id]['purchase_year']\n",
    "                        profit = agent.update_portfolio(prop_id, 'sell', current_value, year)\n",
    "                        total_profit += profit\n",
    "\n",
    "                        # Record the transaction\n",
    "                        transactions.append({\n",
    "                            'year': year,\n",
    "                            'action': 'sell',\n",
    "                            'property_id': prop_id,\n",
    "                            'price': current_value,\n",
    "                            'profit': profit,\n",
    "                            'holding_period': year - purchase_year\n",
    "                        })\n",
    "\n",
    "                        acted_properties.add(prop_id)\n",
    "\n",
    "        # Then, consider buying new properties\n",
    "        for _, row in year_data.iterrows():\n",
    "            prop_id = row['zpid']\n",
    "\n",
    "            # Skip if we already acted on this property this year\n",
    "            if prop_id in acted_properties:\n",
    "                continue\n",
    "\n",
    "            # Get property state\n",
    "            prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "            if prop_state is not None:\n",
    "                # Get agent's action (0: hold/don't buy, 1: buy) - in eval mode\n",
    "                action = agent.act(prop_state, eval_mode=True)\n",
    "\n",
    "                if action == 1:  # Buy\n",
    "                    property_price = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "                    if property_price is not None and agent.cash >= property_price:\n",
    "                        success = agent.update_portfolio(prop_id, 'buy', property_price, year)\n",
    "\n",
    "                        if success:\n",
    "                            # Record the transaction\n",
    "                            transactions.append({\n",
    "                                'year': year,\n",
    "                                'action': 'buy',\n",
    "                                'property_id': prop_id,\n",
    "                                'price': property_price\n",
    "                            })\n",
    "\n",
    "                            acted_properties.add(prop_id)\n",
    "\n",
    "        # Update agent's net worth at the end of each year\n",
    "        current_property_values = {prop_id: data_processor.get_property_value(prop_id, year)\n",
    "                                  for prop_id in agent.portfolio.keys()}\n",
    "        current_property_values = {k: v for k, v in current_property_values.items() if v is not None}\n",
    "\n",
    "        net_worth = agent.record_net_worth(current_property_values)\n",
    "        print(f\"Year {year} - Net Worth: ${net_worth:,.2f}, Cash: ${agent.cash:,.2f}, Properties: {len(agent.portfolio)}\")\n",
    "\n",
    "    # Calculate final return\n",
    "    final_return = agent.net_worth_history[-1] - 1000000  # Final net worth minus initial cash\n",
    "    roi = (final_return / 1000000) * 100  # ROI as percentage\n",
    "\n",
    "    print(f\"\\nEvaluation Results on {split} split:\")\n",
    "    print(f\"Final Net Worth: ${agent.net_worth_history[-1]:,.2f}\")\n",
    "    print(f\"Total Return: ${final_return:,.2f}\")\n",
    "    print(f\"ROI: {roi:.2f}%\")\n",
    "    print(f\"Total Transactions: {len(transactions)}\")\n",
    "\n",
    "    # Convert transactions to DataFrame for analysis\n",
    "    if transactions:\n",
    "        transactions_df = pd.DataFrame(transactions)\n",
    "        buy_count = len(transactions_df[transactions_df['action'] == 'buy'])\n",
    "        sell_count = len(transactions_df[transactions_df['action'] == 'sell'])\n",
    "        avg_profit = transactions_df[transactions_df['action'] == 'sell']['profit'].mean()\n",
    "        avg_holding = transactions_df[transactions_df['action'] == 'sell']['holding_period'].mean()\n",
    "\n",
    "        print(f\"Buy Transactions: {buy_count}\")\n",
    "        print(f\"Sell Transactions: {sell_count}\")\n",
    "        print(f\"Average Profit per Sale: ${avg_profit:,.2f}\")\n",
    "        print(f\"Average Holding Period: {avg_holding:.1f} years\")\n",
    "\n",
    "    # Plot net worth over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(years[:len(agent.net_worth_history)], agent.net_worth_history)\n",
    "    plt.title(f'Agent Net Worth Over Time ({split} split)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Net Worth ($)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'agent_net_worth_{split}.png')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'final_net_worth': agent.net_worth_history[-1],\n",
    "        'total_return': final_return,\n",
    "        'roi': roi,\n",
    "        'transactions': transactions\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the complete workflow\"\"\"\n",
    "    # Initialize data processor\n",
    "    data_processor = RealEstateDataProcessor(data_dir='./')\n",
    "\n",
    "    # Load and process data\n",
    "    if not data_processor.load_datasets():\n",
    "        print(\"Failed to load datasets. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Split data by properties\n",
    "    data_processor.split_data_by_properties()\n",
    "\n",
    "    # Engineer features\n",
    "    data_processor.engineer_features()\n",
    "    print(\"TWAS SPLIT\")\n",
    "    # Prepare state features\n",
    "    _, state_size = data_processor.prepare_state_features()\n",
    "    print(\"TWAS PREPARED\")\n",
    "    # Define action space: 0 = Hold/Don't Buy, 1 = Buy/Sell\n",
    "    action_size = 2\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = RealEstateAgent(state_size=state_size, action_size=action_size)\n",
    "    print(\"HEEEEEEEEEEEERE\")\n",
    "    # Train the agent\n",
    "    train_returns = train_agent(agent, data_processor, split='train', num_epochs=5)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_agent(agent, data_processor, split='val')\n",
    "\n",
    "    # If validation results are satisfactory, evaluate on test set\n",
    "    test_results = evaluate_agent(agent, data_processor, split='test')\n",
    "\n",
    "    # Finally, evaluate on production set\n",
    "    prod_results = evaluate_agent(agent, data_processor, split='prod')\n",
    "\n",
    "    # Compare results across splits\n",
    "    results = {\n",
    "        'val': val_results,\n",
    "        'test': test_results,\n",
    "        'prod': prod_results\n",
    "    }\n",
    "\n",
    "    # Print comparison\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"Split\\tROI\\tFinal Net Worth\")\n",
    "    for split, res in results.items():\n",
    "        print(f\"{split}\\t{res['roi']:.2f}%\\t${res['final_net_worth']:,.2f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
