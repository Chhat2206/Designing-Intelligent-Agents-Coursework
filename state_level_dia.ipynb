{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f464f03f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yousy\\Documents\\Work\\dia\\Wallstreetbets\\.venv\\lib\\site-packages\\torch\\nn\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yousy\\Documents\\Work\\dia\\Wallstreetbets\\.venv\\lib\\site-packages\\torch\\nn\\parameter.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disabled_torch_function_impl\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_ParameterMeta\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TensorMeta):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _C: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define experience tuple structure\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer to store and sample agent experiences\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
    "        experiences = random.sample(self.memory, k=min(self.batch_size, len(self.memory)))\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for real estate decision making\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_layers=[256, 128, 64]):\n",
    "        \"\"\"Initialize network parameters and build model\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            hidden_layers (list): List of hidden layer sizes\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            self.layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_layers[-1], action_size))\n",
    "\n",
    "        # Batch normalization for better training stability\n",
    "        self.batch_norm_layers = nn.ModuleList([nn.BatchNorm1d(size) for size in hidden_layers])\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values\"\"\"\n",
    "        x = state\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.batch_norm_layers[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "class RealEstateAgent:\n",
    "    \"\"\"Agent that interacts with and learns from the real estate environment\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_layers=[256, 128, 64], seed=42):\n",
    "        \"\"\"Initialize an Agent object\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            hidden_layers (list): List of hidden layer sizes\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, hidden_layers).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, hidden_layers).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=5e-4)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size=10000, batch_size=64)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99    # discount factor\n",
    "        self.tau = 1e-3      # for soft update of target parameters\n",
    "        self.update_every = 4  # how often to update the network\n",
    "        self.batch_size = 64   # minibatch size\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = 1.0     # exploration rate\n",
    "        self.epsilon_decay = 0.995  # decay rate\n",
    "        self.epsilon_min = 0.01     # minimum exploration rate\n",
    "\n",
    "        # Portfolio tracking\n",
    "        self.portfolio = {}    # Dictionary to track owned properties\n",
    "        self.cash = 1000000    # Starting cash (can be adjusted)\n",
    "        self.net_worth_history = []  # Track net worth over time\n",
    "\n",
    "        # Holding period preferences based on net worth\n",
    "        self.net_worth_threshold_1 = 2000000  # First threshold\n",
    "        self.net_worth_threshold_2 = 5000000  # Second threshold\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample to learn\"\"\"\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def act(self, state, eval_mode=False):\n",
    "        \"\"\"Returns actions for given state as per current policy\n",
    "\n",
    "        Args:\n",
    "            state (array_like): Current state\n",
    "            eval_mode (bool): Whether to use evaluation mode (no exploration)\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(state, np.ndarray) and state.dtype == np.dtype('O'):\n",
    "          state = np.vstack(state).astype(np.float32)\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if eval_mode:\n",
    "            epsilon = 0.0  # No exploration in evaluation mode\n",
    "        else:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples\n",
    "\n",
    "        Args:\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "\n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters:\n",
    "        ÃŽÂ¸_target = Ãâ€ž*ÃŽÂ¸_local + (1 - Ãâ€ž)*ÃŽÂ¸_target\n",
    "\n",
    "        Args:\n",
    "            local_model (PyTorch model): source model\n",
    "            target_model (PyTorch model): target model\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def update_portfolio(self, property_id, action_type, price, year):\n",
    "        \"\"\"Update agent's portfolio based on actions taken\n",
    "\n",
    "        Args:\n",
    "            property_id (str): Property ID\n",
    "            action_type (str): 'buy' or 'sell'\n",
    "            price (float): Transaction price\n",
    "            year (int): Transaction year\n",
    "        \"\"\"\n",
    "        if action_type == 'buy':\n",
    "            if self.cash >= price:\n",
    "                self.portfolio[property_id] = {\n",
    "                    'purchase_year': year,\n",
    "                    'purchase_price': price,\n",
    "                    'recommended_holding_period': self.get_recommended_holding_period()\n",
    "                }\n",
    "                self.cash -= price\n",
    "                return True\n",
    "            else:\n",
    "                return False  # Not enough cash\n",
    "\n",
    "        elif action_type == 'sell':\n",
    "            if property_id in self.portfolio:\n",
    "                self.cash += price\n",
    "                purchased_price = self.portfolio[property_id]['purchase_price']\n",
    "                profit = price - purchased_price\n",
    "                del self.portfolio[property_id]\n",
    "                return profit\n",
    "            else:\n",
    "                return 0  # Property not in portfolio\n",
    "\n",
    "    def get_recommended_holding_period(self):\n",
    "        \"\"\"Determine recommended holding period based on net worth\"\"\"\n",
    "        net_worth = self.calculate_net_worth()\n",
    "\n",
    "        if net_worth < self.net_worth_threshold_1:\n",
    "            # Early phase: shorter holding periods\n",
    "            return random.randint(2, 3)\n",
    "        elif net_worth < self.net_worth_threshold_2:\n",
    "            # Middle phase: medium holding periods\n",
    "            return random.randint(3, 4)\n",
    "        else:\n",
    "            # High net worth phase: longer holding periods\n",
    "            return random.randint(4, 5)\n",
    "\n",
    "    def calculate_net_worth(self, current_property_values=None):\n",
    "        \"\"\"Calculate current net worth including cash and property values\"\"\"\n",
    "        if current_property_values is None:\n",
    "            # If no property values provided, use purchase prices\n",
    "            property_value_sum = sum(p['purchase_price'] for p in self.portfolio.values())\n",
    "        else:\n",
    "            # Use provided current values\n",
    "            property_value_sum = sum(current_property_values.get(pid, p['purchase_price'])\n",
    "                                    for pid, p in self.portfolio.items())\n",
    "\n",
    "        return self.cash + property_value_sum\n",
    "\n",
    "    def record_net_worth(self, current_property_values):\n",
    "        \"\"\"Record current net worth for tracking\"\"\"\n",
    "        net_worth = self.calculate_net_worth(current_property_values)\n",
    "        self.net_worth_history.append(net_worth)\n",
    "        return net_worth\n",
    "\n",
    "    def display_portfolio_details(self, data_processor):\n",
    "        \"\"\"Display detailed information about properties in the portfolio\"\"\"\n",
    "        print(\"\\n========== PORTFOLIO DETAILS ==========\")\n",
    "        print(f\"Total Properties: {len(self.portfolio)}\")\n",
    "        print(f\"Cash Balance: ${self.cash:,.2f}\")\n",
    "\n",
    "        if not self.portfolio:\n",
    "            print(\"No properties currently in portfolio.\")\n",
    "            return\n",
    "\n",
    "        # Get property listings for location data\n",
    "        property_listings = data_processor.datasets['property_listings']\n",
    "\n",
    "        # Set up a table format\n",
    "        headers = [\"ZPID\", \"Location\", \"Purchase Price\", \"Purchase Year\", \"Holding Period\", \"Est. Years to Profit\"]\n",
    "        row_format = \"{:<12} {:<30} {:<15} {:<15} {:<15} {:<20}\"\n",
    "        print(row_format.format(*headers))\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # Display each property\n",
    "        for zpid, details in self.portfolio.items():\n",
    "            # Get property info\n",
    "            property_info = property_listings[property_listings['zpid'] == zpid]\n",
    "\n",
    "            if not property_info.empty:\n",
    "                # Extract location data\n",
    "                city = property_info['city'].values[0] if 'city' in property_info.columns else 'Unknown'\n",
    "                state = property_info['state'].values[0] if 'state' in property_info.columns else 'Unknown'\n",
    "                location = f\"{city}, {state}\"\n",
    "\n",
    "                # Calculate estimated years to profit\n",
    "                est_years_to_profit = self.estimate_years_to_profit(zpid, details, data_processor)\n",
    "\n",
    "                # Format and print the row\n",
    "                row = [\n",
    "                    zpid,\n",
    "                    location,\n",
    "                    f\"${details['purchase_price']:,.2f}\",\n",
    "                    details['purchase_year'],\n",
    "                    details['recommended_holding_period'],\n",
    "                    f\"{est_years_to_profit:.1f} years\"\n",
    "                ]\n",
    "                print(row_format.format(*row))\n",
    "\n",
    "        print(\"=======================================\\n\")\n",
    "\n",
    "    def estimate_years_to_profit(self, property_id, property_details, data_processor):\n",
    "        \"\"\"Estimate years to profitability based on historical price trends\"\"\"\n",
    "        purchase_price = property_details['purchase_price']\n",
    "        purchase_year = property_details['purchase_year']\n",
    "\n",
    "        # Get historical price data for similar properties in the same area\n",
    "        property_info = data_processor.datasets['property_listings'][\n",
    "            data_processor.datasets['property_listings']['zpid'] == property_id]\n",
    "\n",
    "        if property_info.empty:\n",
    "            # Default to recommended holding period if no property info available\n",
    "            return property_details['recommended_holding_period']\n",
    "\n",
    "        # Get price history for this property or similar properties\n",
    "        price_history = data_processor.datasets['price_history']\n",
    "\n",
    "        # Try to get this property's specific price history\n",
    "        property_price_history = price_history[price_history['zpid'] == property_id]\n",
    "\n",
    "        # If no specific history, try to get similar properties (e.g., same city/state)\n",
    "        if len(property_price_history) < 2:  # Need at least 2 points to calculate trend\n",
    "            # Get city and state\n",
    "            city = property_info['city'].values[0] if 'city' in property_info.columns else None\n",
    "            state = property_info['state'].values[0] if 'state' in property_info.columns else None\n",
    "\n",
    "            # Find similar properties\n",
    "            similar_properties = data_processor.datasets['property_listings'][\n",
    "                (data_processor.datasets['property_listings']['city'] == city) &\n",
    "                (data_processor.datasets['property_listings']['state'] == state)\n",
    "            ]['zpid'].unique()\n",
    "\n",
    "            # Get price history for similar properties\n",
    "            property_price_history = price_history[price_history['zpid'].isin(similar_properties)]\n",
    "\n",
    "        # Calculate average annual appreciation rate from historical data\n",
    "        if len(property_price_history) >= 2:\n",
    "            property_price_history['dateOfEvent'] = pd.to_datetime(property_price_history['dateOfEvent'])\n",
    "            property_price_history = property_price_history.sort_values('dateOfEvent')\n",
    "\n",
    "            # Group by year and get average price\n",
    "            yearly_prices = property_price_history.groupby(\n",
    "                property_price_history['dateOfEvent'].dt.year)['price'].mean()\n",
    "\n",
    "            if len(yearly_prices) >= 2:\n",
    "                # Calculate year-over-year growth rates\n",
    "                yearly_growth_rates = yearly_prices.pct_change().dropna()\n",
    "\n",
    "                # Get average annual growth rate\n",
    "                avg_growth_rate = yearly_growth_rates.mean()\n",
    "\n",
    "                # Ensure we have a valid growth rate\n",
    "                if pd.notnull(avg_growth_rate) and avg_growth_rate > 0:\n",
    "                    # Calculate years to profit based on growth rate\n",
    "                    # Formula: FV = PV * (1 + r)^t â†’ solve for t\n",
    "                    # t = log(FV/PV) / log(1 + r)\n",
    "                    # We want FV to be at least 10% more than PV for profit (after costs)\n",
    "                    target_value = purchase_price * 1.1  # 10% profit\n",
    "                    years_to_profit = np.log(target_value / purchase_price) / np.log(1 + avg_growth_rate)\n",
    "                    return max(1, years_to_profit)  # Minimum 1 year\n",
    "\n",
    "        # Default fallback: use recommended holding period\n",
    "        return property_details['recommended_holding_period']\n",
    "\n",
    "class RealEstateDataProcessor:\n",
    "    \"\"\"Class to handle real estate data loading, processing and feature engineering\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir='./'):\n",
    "        \"\"\"Initialize the data processor\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory containing the dataset files\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.scaler = None\n",
    "        self.datasets = {}\n",
    "        self.property_sets = {}  # For train/val/test/prod splits\n",
    "        self.last_updated_years = {}  # Track when each property was last updated\n",
    "        self.property_values_by_year = {}  # Track property values by year\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load all dataset files\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "\n",
    "        try:\n",
    "            self.datasets = {}\n",
    "\n",
    "            self.datasets['mortgage_info'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_mortgage_info.csv')))\n",
    "            self.datasets['nearby_homes'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_nearby_homes.csv')))\n",
    "            self.datasets['price_history'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_price_history.csv')))\n",
    "            self.datasets['schools_info'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_schools_info.csv')))\n",
    "            self.datasets['subtype'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_subtype.csv')))\n",
    "            self.datasets['tax_info'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'listing_tax_info.csv')))\n",
    "            self.datasets['property_listings'] = reduce_memory_usage(pd.read_csv(os.path.join(self.data_dir, 'property_listings.csv')))\n",
    "\n",
    "            print(\"All datasets loaded successfully!\")\n",
    "            print(f\"Main property dataset shape: {self.datasets['property_listings'].shape}\")\n",
    "\n",
    "            # Extract historical years from price_history\n",
    "            self.datasets['price_history']['dateOfEvent'] = pd.to_datetime(self.datasets['price_history']['dateOfEvent'])\n",
    "            price_history_years = self.datasets['price_history']['dateOfEvent'].dt.year\n",
    "\n",
    "            # Filter for years 2000 and later\n",
    "            valid_years = sorted(price_history_years.unique())\n",
    "            valid_years = [year for year in valid_years if year >= 2000]\n",
    "\n",
    "            # Extract current property years from lastUpdated\n",
    "            self.datasets['property_listings']['year'] = pd.to_datetime(\n",
    "                self.datasets['property_listings']['lastUpdated']).dt.year\n",
    "\n",
    "            # Create a mapping of zpid to all its available years from both datasets\n",
    "            property_years = {}\n",
    "\n",
    "            # First add years from price_history (historical data)\n",
    "            for zpid, year in zip(self.datasets['price_history']['zpid'],\n",
    "                                  self.datasets['price_history']['dateOfEvent'].dt.year):\n",
    "                if year >= 2000:  # Only include years from 2000 onwards\n",
    "                    if zpid not in property_years:\n",
    "                        property_years[zpid] = set()\n",
    "                    property_years[zpid].add(year)\n",
    "\n",
    "            # Then add current years from property_listings\n",
    "            for zpid, year in zip(self.datasets['property_listings']['zpid'],\n",
    "                                  self.datasets['property_listings']['year']):\n",
    "                if zpid in property_years:  # Only add if property exists in historical data\n",
    "                    property_years[zpid].add(year)\n",
    "\n",
    "            # Use all years from 2000 to present\n",
    "            self.years = sorted([year for year in valid_years if year >= 2000])\n",
    "\n",
    "            print(f\"Dataset spans years: {min(self.years)} to {max(self.years)}\")\n",
    "            print(f\"Total years available: {len(self.years)}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading datasets: {e}\")\n",
    "            print(\"Please ensure all required CSV files are in the specified directory.\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def split_data_by_properties(self):\n",
    "        \"\"\"Split data by properties while maintaining the full timeline\n",
    "        Uses property-based split instead of time-based split.\n",
    "        Only considers 30% of properties for all processing.\n",
    "        \"\"\"\n",
    "        if not self.datasets:\n",
    "            print(\"Datasets not loaded. Call load_datasets() first.\")\n",
    "            return\n",
    "\n",
    "        # Get all unique property IDs\n",
    "        all_properties = self.datasets['property_listings']['zpid'].unique()\n",
    "        print(f\"Total unique properties: {len(all_properties)}\")\n",
    "\n",
    "        # Shuffle the properties for random assignment\n",
    "        np.random.shuffle(all_properties)\n",
    "        \n",
    "        # Select only 30% of properties for consideration\n",
    "        considered_count = int(len(all_properties) * 0.3)\n",
    "        considered_properties = all_properties[:considered_count]\n",
    "        print(f\"Considering only {considered_count} properties ({considered_count/len(all_properties):.1%} of total)\")\n",
    "        \n",
    "        # Split the considered 30% into train (60%), validation (15%), test (15%), and production (10%)\n",
    "        train_size = int(len(considered_properties) * 0.15)\n",
    "        val_size = int(len(considered_properties) * 0.15)\n",
    "        test_size = int(len(considered_properties) * 0.15)\n",
    "\n",
    "        self.property_sets = {\n",
    "            'train': considered_properties[:train_size],\n",
    "            'val': considered_properties[train_size:train_size + val_size],\n",
    "            'test': considered_properties[train_size + val_size:train_size + val_size + test_size],\n",
    "            'prod': considered_properties[train_size + val_size + test_size:]\n",
    "        }\n",
    "\n",
    "        # Print statistics\n",
    "        for split, props in self.property_sets.items():\n",
    "            print(f\"{split} set: {len(props)} properties ({len(props)/len(all_properties):.1%} of total)\")\n",
    "\n",
    "        return self.property_sets\n",
    "\n",
    "\n",
    "    def engineer_features(self):\n",
    "        \"\"\"Engineer features with focus on historical price data integration\"\"\"\n",
    "        if not self.datasets or not self.property_sets:\n",
    "            print(\"Datasets not loaded or not split. Call load_datasets() and split_data_by_properties() first.\")\n",
    "            return\n",
    "\n",
    "        print(\"Engineering features with historical data integration...\")\n",
    "\n",
    "        # Get all properties from our splits (the 30% considered)\n",
    "        considered_properties = np.concatenate(list(self.property_sets.values()))\n",
    "        \n",
    "        # 1. Start with price history to create a base DataFrame with historical data\n",
    "        price_history = self.datasets['price_history'].copy()\n",
    "        # Filter to only include our considered properties\n",
    "        price_history = price_history[price_history['zpid'].isin(considered_properties)]\n",
    "        \n",
    "        price_history['dateOfEvent'] = pd.to_datetime(price_history['dateOfEvent'])\n",
    "        price_history['year'] = price_history['dateOfEvent'].dt.year\n",
    "\n",
    "        # Filter for years 2000 and later\n",
    "        price_history = price_history[price_history['year'] >= 2000]\n",
    "\n",
    "        # Group by zpid and year to get price per year (if multiple events per year)\n",
    "        yearly_prices = price_history.groupby(['zpid', 'year'])['price'].mean().reset_index()\n",
    "\n",
    "        # Calculate year-over-year price changes\n",
    "        yearly_prices = yearly_prices.sort_values(['zpid', 'year'])\n",
    "        yearly_prices['prev_price'] = yearly_prices.groupby('zpid')['price'].shift(1)\n",
    "        yearly_prices['price_yoy_change'] = (yearly_prices['price'] - yearly_prices['prev_price']) / yearly_prices['prev_price']\n",
    "\n",
    "        # 2. Get property information from property_listings\n",
    "        properties = self.datasets['property_listings'].copy()\n",
    "        # Filter to only include considered properties\n",
    "        properties = properties[properties['zpid'].isin(considered_properties)]\n",
    "        \n",
    "        properties['lastUpdated'] = pd.to_datetime(properties['lastUpdated'])\n",
    "        properties['year_updated'] = properties['lastUpdated'].dt.year\n",
    "\n",
    "        # 3. Create a base DataFrame with all property-year combinations\n",
    "        # Use only our considered properties\n",
    "        all_properties = considered_properties\n",
    "        \n",
    "        # Get all years from 2000 to present\n",
    "        all_years = list(range(2000, max(properties['year_updated'].max(),\n",
    "                                        price_history['year'].max()) + 1))\n",
    "\n",
    "        # Create a cross-product of properties and years\n",
    "        property_years = []\n",
    "        for prop in all_properties:\n",
    "            for year in all_years:\n",
    "                property_years.append({'zpid': prop, 'year': year})\n",
    "\n",
    "        # Create base DataFrame\n",
    "        df = pd.DataFrame(property_years)\n",
    "\n",
    "        # 4. Merge property information\n",
    "        # First get the latest property info\n",
    "        latest_properties = properties.sort_values('lastUpdated').groupby('zpid').last().reset_index()\n",
    "\n",
    "        # Merge with base DataFrame\n",
    "        df = pd.merge(df, latest_properties[['zpid', 'price', 'livingArea', 'bedrooms', 'bathrooms',\n",
    "                                          'yearBuilt', 'homeType', 'homeStatus', 'state',\n",
    "                                          'pageViewCount', 'favoriteCount']],\n",
    "                    on='zpid', how='left')\n",
    "\n",
    "        # 5. Merge historical prices\n",
    "        df = pd.merge(df, yearly_prices[['zpid', 'year', 'price_yoy_change']],\n",
    "                    on=['zpid', 'year'], how='left')\n",
    "\n",
    "        # For historical years, update property price using the price from price_history\n",
    "        historical_prices = pd.merge(df[['zpid', 'year']],\n",
    "                                    yearly_prices[['zpid', 'year', 'price']],\n",
    "                                    on=['zpid', 'year'], how='left')\n",
    "\n",
    "        # Update prices where historical data exists\n",
    "        mask = historical_prices['price'].notna()\n",
    "        if mask.any():\n",
    "            df.loc[mask, 'price'] = historical_prices.loc[mask, 'price'].values\n",
    "\n",
    "        # 6. Add basic features\n",
    "        df.loc[:, 'price_per_sqft'] = df['price'] / df['livingArea']\n",
    "        df.loc[:, 'age'] = df['year'] - df['yearBuilt']\n",
    "        df.loc[:, 'bed_bath_ratio'] = df['bedrooms'] / df['bathrooms']\n",
    "\n",
    "        # 7. Add tax and mortgage features\n",
    "        tax_info = self.datasets['tax_info'][['zpid', 'taxPaid', 'valueIncreaseRate']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        tax_info = tax_info[tax_info['zpid'].isin(considered_properties)]\n",
    "        df = pd.merge(df, tax_info, on='zpid', how='left')\n",
    "\n",
    "        mortgage_info = self.datasets['mortgage_info'][['zpid', 'rate']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        mortgage_info = mortgage_info[mortgage_info['zpid'].isin(considered_properties)]\n",
    "        df = pd.merge(df, mortgage_info, on='zpid', how='left')\n",
    "\n",
    "        # 8. Add school ratings\n",
    "        schools = self.datasets['schools_info'][['zpid', 'schoolRating']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        schools = schools[schools['zpid'].isin(considered_properties)]\n",
    "        school_ratings = schools.groupby('zpid')['schoolRating'].mean().reset_index()\n",
    "        school_ratings.rename(columns={'schoolRating': 'avg_school_rating'}, inplace=True)\n",
    "        df = pd.merge(df, school_ratings, on='zpid', how='left')\n",
    "\n",
    "        # 9. Add nearby home comparison\n",
    "        nearby = self.datasets['nearby_homes'][['zpid', 'priceComp']].copy()\n",
    "        # Filter to only include considered properties\n",
    "        nearby = nearby[nearby['zpid'].isin(considered_properties)]\n",
    "        nearby_avg_price = nearby.groupby('zpid')['priceComp'].mean().reset_index()\n",
    "        nearby_avg_price.rename(columns={'priceComp': 'nearby_avg_price'}, inplace=True)\n",
    "        df = pd.merge(df, nearby_avg_price, on='zpid', how='left')\n",
    "        df.loc[:, 'price_vs_nearby'] = df['price'] / df['nearby_avg_price']\n",
    "        \n",
    "        # 10. One-hot encoding\n",
    "        for col in ['homeType', 'homeStatus', 'state']:\n",
    "            if col in df.columns:\n",
    "                top_values = df[col].value_counts().nlargest(5).index\n",
    "                for val in top_values:\n",
    "                    df.loc[:, f'{col}_{val}'] = (df[col] == val).astype('int8')\n",
    "\n",
    "        # Drop original categorical columns to save memory\n",
    "        df = df.drop(['homeType', 'homeStatus', 'state'], axis=1, errors='ignore')\n",
    "\n",
    "        # 11. Handle missing values\n",
    "        df.fillna({\n",
    "            'price_yoy_change': 0,\n",
    "            'valueIncreaseRate': df['valueIncreaseRate'].median(),\n",
    "            'taxPaid': df['taxPaid'].median(),\n",
    "            'rate': 0.05,  # Default mortgage rate\n",
    "            'avg_school_rating': df['avg_school_rating'].median(),\n",
    "            'nearby_avg_price': df['nearby_avg_price'].median(),\n",
    "            'price_vs_nearby': 1,\n",
    "            'pageViewCount': 0,\n",
    "            'favoriteCount': 0\n",
    "        }, inplace=True)\n",
    "\n",
    "        # 12. Store property values by year for reward calculation\n",
    "        for year in df['year'].unique():\n",
    "            year_data = df[df['year'] == year][['zpid', 'price']]\n",
    "            self.property_values_by_year[year] = dict(zip(year_data['zpid'].values, year_data['price'].values))\n",
    "\n",
    "        # 13. Final cleanup\n",
    "        df.replace([np.inf, -np.inf], np.finfo('float32').max, inplace=True)\n",
    "\n",
    "        # Update main dataframe\n",
    "        self.datasets['processed_data'] = df\n",
    "\n",
    "        print(f\"Feature engineering complete. Final dataframe shape: {df.shape}\")\n",
    "        print(f\"Years available in processed data: {sorted(df['year'].unique())}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def prepare_state_features(self):\n",
    "        \"\"\"Prepare the state features for the DQN model\"\"\"\n",
    "        if 'processed_data' not in self.datasets:\n",
    "            print(\"Features not engineered. Call engineer_features() first.\")\n",
    "            return\n",
    "\n",
    "        df = self.datasets['processed_data']\n",
    "\n",
    "        # Check for and add default values for missing columns\n",
    "        missing_columns = {\n",
    "            'rate': 0.05,  # Default mortgage rate\n",
    "            'pageViewCount': 0,  # Default page views\n",
    "            'favoriteCount': 0   # Default favorites\n",
    "        }\n",
    "\n",
    "        for col, default_val in missing_columns.items():\n",
    "            if col not in df.columns:\n",
    "                print(f\"Warning: Column '{col}' not found, adding with default value {default_val}\")\n",
    "                df[col] = default_val\n",
    "\n",
    "        # Now proceed with feature selection\n",
    "        feature_columns = [\n",
    "            'price', 'livingArea', 'bedrooms', 'bathrooms', 'yearBuilt', 'age',\n",
    "            'price_per_sqft', 'bed_bath_ratio', 'price_yoy_change', 'taxPaid',\n",
    "            'valueIncreaseRate', 'rate', 'avg_school_rating', 'nearby_avg_price',\n",
    "            'price_vs_nearby', 'pageViewCount', 'favoriteCount'\n",
    "        ]\n",
    "        # Add the one-hot encoded columns for homeType, homeStatus, and state\n",
    "        one_hot_columns = [col for col in df.columns if col.startswith(('homeType_', 'homeStatus_', 'state_'))]\n",
    "        feature_columns.extend(one_hot_columns)\n",
    "\n",
    "        # Extract just the feature columns\n",
    "        features_df = df[feature_columns + ['zpid', 'year']]\n",
    "\n",
    "        # Scale the numerical features\n",
    "        numerical_features = [\n",
    "            'price', 'livingArea', 'bedrooms', 'bathrooms', 'age',\n",
    "            'price_per_sqft', 'bed_bath_ratio', 'price_yoy_change', 'taxPaid',\n",
    "            'valueIncreaseRate', 'rate', 'avg_school_rating', 'nearby_avg_price',\n",
    "            'price_vs_nearby', 'pageViewCount', 'favoriteCount'\n",
    "        ]\n",
    "\n",
    "        # Fit scaler on training data only to prevent data leakage\n",
    "        train_features = features_df[features_df['zpid'].isin(self.property_sets['train'])]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train_features[numerical_features])\n",
    "\n",
    "        # Transform all features\n",
    "        features_df[numerical_features] = self.scaler.transform(features_df[numerical_features])\n",
    "\n",
    "        # Store the prepared features\n",
    "        self.datasets['state_features'] = features_df\n",
    "\n",
    "        # Return the feature dimension for the DQN model\n",
    "        self.state_size = len(feature_columns)\n",
    "        print(f\"State features prepared. State dimension: {self.state_size}\")\n",
    "\n",
    "        return features_df, self.state_size\n",
    "\n",
    "    def get_properties_for_year(self, year, split):\n",
    "        \"\"\"Get properties available in a specific year for a specific data split\"\"\"\n",
    "        if 'state_features' not in self.datasets:\n",
    "            print(\"State features not prepared. Call prepare_state_features() first.\")\n",
    "            return None\n",
    "\n",
    "        # Get properties for the given split\n",
    "        split_properties = self.property_sets[split]\n",
    "\n",
    "        # Get data for the given year filtered by split properties\n",
    "        year_data = self.datasets['state_features'][\n",
    "            (self.datasets['state_features']['year'] == year) &\n",
    "            (self.datasets['state_features']['zpid'].isin(split_properties))\n",
    "        ]\n",
    "\n",
    "        return year_data\n",
    "\n",
    "    def get_property_state(self, property_id, year):\n",
    "        \"\"\"Get the state representation for a specific property and year\"\"\"\n",
    "        if 'state_features' not in self.datasets:\n",
    "            print(\"State features not prepared. Call prepare_state_features() first.\")\n",
    "            return None\n",
    "\n",
    "        # Get data for the specific property and year\n",
    "        property_data = self.datasets['state_features'][\n",
    "            (self.datasets['state_features']['zpid'] == property_id) &\n",
    "            (self.datasets['state_features']['year'] == year)\n",
    "        ]\n",
    "\n",
    "        if property_data.empty:\n",
    "            return None\n",
    "\n",
    "        # Return state features (excluding zpid and year)\n",
    "        state_features = property_data.drop(['zpid', 'year'], axis=1).values[0]\n",
    "\n",
    "        return state_features\n",
    "\n",
    "    def get_property_value(self, property_id, year):\n",
    "        \"\"\"Get the value of a property in a specific year\"\"\"\n",
    "        return self.property_values_by_year.get(year, {}).get(property_id, None)\n",
    "\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\"Reduces memory usage of a dataframe by downcasting numeric types.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage before optimization: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Skip categorical columns or convert them to ordered\n",
    "        if pd.api.types.is_categorical_dtype(col_type):\n",
    "            # Option 1: Skip categorical columns entirely\n",
    "            continue\n",
    "\n",
    "            # Option 2: Make categorical columns ordered before operations\n",
    "            # df[col] = df[col].cat.as_ordered()\n",
    "\n",
    "        elif col_type != object:\n",
    "            # Use safe min/max operations with explicit null handling\n",
    "            if df[col].isna().any():  # Check for nulls first\n",
    "                non_null_values = df[col].dropna()\n",
    "                if len(non_null_values) > 0:  # Make sure we have values after dropping nulls\n",
    "                    c_min = non_null_values.min()\n",
    "                    c_max = non_null_values.max()\n",
    "                else:\n",
    "                    # If all values are null, use safe defaults\n",
    "                    c_min = 0\n",
    "                    c_max = 0\n",
    "            else:\n",
    "                # No nulls, proceed normally\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "            # Integer downcasting\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "\n",
    "            # Float downcasting\n",
    "            elif str(col_type)[:5] == 'float':\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        # For object columns that we want to convert to category\n",
    "        elif df[col].nunique() < df.shape[0] * 0.5:\n",
    "            # Handle null values in categorical data\n",
    "            if df[col].isna().any():\n",
    "                # Convert to object type first, fill nulls with placeholder, then convert to category\n",
    "                df[col] = df[col].astype(object).fillna('Unknown').astype('category')\n",
    "            else:\n",
    "                # No nulls, safe to convert directly\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization: {end_mem:.2f} MB')\n",
    "    print(f'Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_agent(agent, data_processor, split='train', num_epochs=5):\n",
    "    \"\"\"Train the agent using the specified data split\"\"\"\n",
    "    print(f\"Training agent on {split} split for {num_epochs} epochs...\")\n",
    "\n",
    "    years = data_processor.years\n",
    "    returns_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Reset agent's portfolio at the beginning of each epoch\n",
    "        agent.portfolio = {}\n",
    "        agent.cash = 1000000  # Reset starting cash\n",
    "        agent.net_worth_history = []\n",
    "\n",
    "        total_profit = 0\n",
    "\n",
    "        # For each year in chronological order\n",
    "        for i, year in enumerate(tqdm(years, desc=f\"Processing years\")):\n",
    "            # Get available properties for this year and split\n",
    "            year_data = data_processor.get_properties_for_year(year, split)\n",
    "\n",
    "            if year_data is None or year_data.empty:\n",
    "                print(f\"No data available for year {year}, split {split}\")\n",
    "                continue\n",
    "\n",
    "            # Track properties acted on this year to avoid duplicates\n",
    "            acted_properties = set()\n",
    "\n",
    "            # First, check if any properties in the portfolio should be sold\n",
    "            properties_to_sell = []\n",
    "            for prop_id, details in agent.portfolio.items():\n",
    "                holding_years = year - details['purchase_year']\n",
    "\n",
    "                # Check if we've held the property for the recommended period\n",
    "                if holding_years >= details['recommended_holding_period']:\n",
    "                    properties_to_sell.append(prop_id)\n",
    "\n",
    "            # Sell properties that have reached their holding period\n",
    "            for prop_id in properties_to_sell:\n",
    "                current_value = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "                if current_value is not None:\n",
    "                    # Get property state for the selling decision\n",
    "                    prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "                    if prop_state is not None:\n",
    "                        # Get agent's action (0: hold, 1: sell)\n",
    "                        action = agent.act(prop_state)\n",
    "\n",
    "                        if action == 1:  # Sell\n",
    "                            profit = agent.update_portfolio(prop_id, 'sell', current_value, year)\n",
    "                            total_profit += profit\n",
    "\n",
    "                            # Calculate reward (profit as percentage of purchase price)\n",
    "                            purchase_price = agent.portfolio.get(prop_id, {}).get('purchase_price', current_value)\n",
    "                            reward = profit / purchase_price if purchase_price > 0 else 0\n",
    "\n",
    "                            # Get the next state (could be None at the end of data)\n",
    "                            next_state = prop_state  # Simplified; in real scenario would be updated state\n",
    "\n",
    "                            # Determine if this is the end of the episode\n",
    "                            done = (i == len(years) - 1)\n",
    "\n",
    "                            # Agent learning step\n",
    "                            agent.step(prop_state, np.array([action]), reward, next_state, done)\n",
    "\n",
    "                            acted_properties.add(prop_id)\n",
    "\n",
    "            # Then, consider buying new properties\n",
    "            for _, row in year_data.iterrows():\n",
    "                prop_id = row['zpid']\n",
    "\n",
    "                # Skip if we already acted on this property this year\n",
    "                if prop_id in acted_properties:\n",
    "                    continue\n",
    "\n",
    "                # Get property state\n",
    "                prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "                if prop_state is not None:\n",
    "                    # Get agent's action (0: hold/don't buy, 1: buy)\n",
    "                    action = agent.act(prop_state)\n",
    "\n",
    "                    if action == 1:  # Buy\n",
    "                        property_price = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "                        if property_price is not None and agent.cash >= property_price:\n",
    "                            success = agent.update_portfolio(prop_id, 'buy', property_price, year)\n",
    "\n",
    "                            # Calculate immediate reward (small negative for buying to account for transaction costs)\n",
    "                            reward = -0.01  # Small negative reward for buying\n",
    "\n",
    "                            # Get the next state (could be None at the end of data)\n",
    "                            next_state = prop_state  # Simplified\n",
    "\n",
    "                            # Determine if this is the end of the episode\n",
    "                            done = (i == len(years) - 1)\n",
    "\n",
    "                            # Agent learning step\n",
    "                            agent.step(prop_state, np.array([[action]]), reward, next_state, done)\n",
    "\n",
    "                            acted_properties.add(prop_id)\n",
    "\n",
    "            # Update agent's net worth at the end of each year\n",
    "            current_property_values = {prop_id: data_processor.get_property_value(prop_id, year)\n",
    "                                      for prop_id in agent.portfolio.keys()}\n",
    "            current_property_values = {k: v for k, v in current_property_values.items() if v is not None}\n",
    "\n",
    "            net_worth = agent.record_net_worth(current_property_values)\n",
    "            print(f\"Year {year} - Net Worth: ${net_worth:,.2f}, Cash: ${agent.cash:,.2f}, Properties: {len(agent.portfolio)}\")\n",
    "\n",
    "        epoch_return = agent.net_worth_history[-1] - 1000000  # Final net worth minus initial cash\n",
    "        returns_history.append(epoch_return)\n",
    "        print(f\"Epoch {epoch+1} Return: ${epoch_return:,.2f}\")\n",
    "\n",
    "        # Display detailed portfolio information after each epoch\n",
    "        agent.display_portfolio_details(data_processor)\n",
    "\n",
    "    return returns_history\n",
    "\n",
    "def evaluate_agent(agent, data_processor, split='test'):\n",
    "    \"\"\"Evaluate the agent on the specified data split\n",
    "\n",
    "    Args:\n",
    "        agent (RealEstateAgent): The trained agent\n",
    "        data_processor (RealEstateDataProcessor): The data processor\n",
    "        split (str): The data split to use ('val', 'test', 'prod')\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating agent on {split} split...\")\n",
    "\n",
    "    years = data_processor.years\n",
    "\n",
    "    # Reset agent's portfolio for evaluation\n",
    "    agent.portfolio = {}\n",
    "    agent.cash = 1000000  # Reset starting cash\n",
    "    agent.net_worth_history = []\n",
    "\n",
    "    total_profit = 0\n",
    "    transactions = []\n",
    "\n",
    "    # For each year in chronological order\n",
    "    for i, year in enumerate(tqdm(years, desc=f\"Evaluating years\")):\n",
    "        # Get available properties for this year and split\n",
    "        year_data = data_processor.get_properties_for_year(year, split)\n",
    "\n",
    "        if year_data is None or year_data.empty:\n",
    "            print(f\"No data available for year {year}, split {split}\")\n",
    "            continue\n",
    "\n",
    "        # Track properties acted on this year to avoid duplicates\n",
    "        acted_properties = set()\n",
    "\n",
    "        # First, check if any properties in the portfolio should be sold\n",
    "        properties_to_sell = []\n",
    "        for prop_id, details in agent.portfolio.items():\n",
    "            holding_years = year - details['purchase_year']\n",
    "\n",
    "            # Check if we've held the property for the recommended period\n",
    "            if holding_years >= details['recommended_holding_period']:\n",
    "                properties_to_sell.append(prop_id)\n",
    "\n",
    "        # Sell properties that have reached their holding period\n",
    "        for prop_id in properties_to_sell:\n",
    "            current_value = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "            if current_value is not None:\n",
    "                # Get property state for the selling decision\n",
    "                prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "                if prop_state is not None:\n",
    "                    # Get agent's action (0: hold, 1: sell) - in eval mode\n",
    "                    action = agent.act(prop_state, eval_mode=True)\n",
    "\n",
    "                    if action == 1:  # Sell\n",
    "                        purchase_price = agent.portfolio[prop_id]['purchase_price']\n",
    "                        purchase_year = agent.portfolio[prop_id]['purchase_year']\n",
    "                        profit = agent.update_portfolio(prop_id, 'sell', current_value, year)\n",
    "                        total_profit += profit\n",
    "\n",
    "                        # Record the transaction\n",
    "                        transactions.append({\n",
    "                            'year': year,\n",
    "                            'action': 'sell',\n",
    "                            'property_id': prop_id,\n",
    "                            'price': current_value,\n",
    "                            'profit': profit,\n",
    "                            'holding_period': year - purchase_year\n",
    "                        })\n",
    "\n",
    "                        acted_properties.add(prop_id)\n",
    "\n",
    "        # Then, consider buying new properties\n",
    "        for _, row in year_data.iterrows():\n",
    "            prop_id = row['zpid']\n",
    "\n",
    "            # Skip if we already acted on this property this year\n",
    "            if prop_id in acted_properties:\n",
    "                continue\n",
    "\n",
    "            # Get property state\n",
    "            prop_state = data_processor.get_property_state(prop_id, year)\n",
    "\n",
    "            if prop_state is not None:\n",
    "                # Get agent's action (0: hold/don't buy, 1: buy) - in eval mode\n",
    "                action = agent.act(prop_state, eval_mode=True)\n",
    "\n",
    "                if action == 1:  # Buy\n",
    "                    property_price = data_processor.get_property_value(prop_id, year)\n",
    "\n",
    "                    if property_price is not None and agent.cash >= property_price:\n",
    "                        success = agent.update_portfolio(prop_id, 'buy', property_price, year)\n",
    "\n",
    "                        if success:\n",
    "                            # Record the transaction\n",
    "                            transactions.append({\n",
    "                                'year': year,\n",
    "                                'action': 'buy',\n",
    "                                'property_id': prop_id,\n",
    "                                'price': property_price\n",
    "                            })\n",
    "\n",
    "                            acted_properties.add(prop_id)\n",
    "\n",
    "        # Update agent's net worth at the end of each year\n",
    "        current_property_values = {prop_id: data_processor.get_property_value(prop_id, year)\n",
    "                                  for prop_id in agent.portfolio.keys()}\n",
    "        current_property_values = {k: v for k, v in current_property_values.items() if v is not None}\n",
    "\n",
    "        net_worth = agent.record_net_worth(current_property_values)\n",
    "        print(f\"Year {year} - Net Worth: ${net_worth:,.2f}, Cash: ${agent.cash:,.2f}, Properties: {len(agent.portfolio)}\")\n",
    "\n",
    "    # Calculate final return\n",
    "    final_return = agent.net_worth_history[-1] - 1000000  # Final net worth minus initial cash\n",
    "    roi = (final_return / 1000000) * 100  # ROI as percentage\n",
    "\n",
    "    print(f\"\\nEvaluation Results on {split} split:\")\n",
    "    print(f\"Final Net Worth: ${agent.net_worth_history[-1]:,.2f}\")\n",
    "    print(f\"Total Return: ${final_return:,.2f}\")\n",
    "    print(f\"ROI: {roi:.2f}%\")\n",
    "    print(f\"Total Transactions: {len(transactions)}\")\n",
    "\n",
    "    # Convert transactions to DataFrame for analysis\n",
    "    if transactions:\n",
    "        transactions_df = pd.DataFrame(transactions)\n",
    "        buy_count = len(transactions_df[transactions_df['action'] == 'buy'])\n",
    "        sell_count = len(transactions_df[transactions_df['action'] == 'sell'])\n",
    "        avg_profit = transactions_df[transactions_df['action'] == 'sell']['profit'].mean()\n",
    "        avg_holding = transactions_df[transactions_df['action'] == 'sell']['holding_period'].mean()\n",
    "\n",
    "        print(f\"Buy Transactions: {buy_count}\")\n",
    "        print(f\"Sell Transactions: {sell_count}\")\n",
    "        print(f\"Average Profit per Sale: ${avg_profit:,.2f}\")\n",
    "        print(f\"Average Holding Period: {avg_holding:.1f} years\")\n",
    "\n",
    "    # Plot net worth over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(years[:len(agent.net_worth_history)], agent.net_worth_history)\n",
    "    plt.title(f'Agent Net Worth Over Time ({split} split)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Net Worth ($)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'agent_net_worth_{split}.png')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'final_net_worth': agent.net_worth_history[-1],\n",
    "        'total_return': final_return,\n",
    "        'roi': roi,\n",
    "        'transactions': transactions\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the complete workflow\"\"\"\n",
    "    # Initialize data processor\n",
    "    data_processor = RealEstateDataProcessor(data_dir='./')\n",
    "\n",
    "    # Load and process data\n",
    "    if not data_processor.load_datasets():\n",
    "        print(\"Failed to load datasets. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Split data by properties\n",
    "    data_processor.split_data_by_properties()\n",
    "\n",
    "    # Engineer features\n",
    "    data_processor.engineer_features()\n",
    "    print(\"TWAS SPLIT\")\n",
    "    # Prepare state features\n",
    "    _, state_size = data_processor.prepare_state_features()\n",
    "    print(\"TWAS PREPARED\")\n",
    "    # Define action space: 0 = Hold/Don't Buy, 1 = Buy/Sell\n",
    "    action_size = 2\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = RealEstateAgent(state_size=state_size, action_size=action_size)\n",
    "    print(\"HEEEEEEEEEEEERE\")\n",
    "    # Train the agent\n",
    "    train_returns = train_agent(agent, data_processor, split='train', num_epochs=5)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_agent(agent, data_processor, split='val')\n",
    "\n",
    "    # If validation results are satisfactory, evaluate on test set\n",
    "    test_results = evaluate_agent(agent, data_processor, split='test')\n",
    "\n",
    "    # Finally, evaluate on production set\n",
    "    prod_results = evaluate_agent(agent, data_processor, split='prod')\n",
    "\n",
    "    # Compare results across splits\n",
    "    results = {\n",
    "        'val': val_results,\n",
    "        'test': test_results,\n",
    "        'prod': prod_results\n",
    "    }\n",
    "\n",
    "    # Print comparison\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"Split\\tROI\\tFinal Net Worth\")\n",
    "    for split, res in results.items():\n",
    "        print(f\"{split}\\t{res['roi']:.2f}%\\t${res['final_net_worth']:,.2f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
